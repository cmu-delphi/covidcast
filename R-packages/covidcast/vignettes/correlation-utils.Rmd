---
title: Correlation utilities
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{2. Correlation utilities}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The covidcast package provides some simple utilities for exploring the
correlations between two signals, over space or time, which may be helpful for
simple analyses and explorations of data.

For these examples, we'll load confirmed cases and deaths to compare against,
and restrict our analysis to counties with at least 500 total cases by August
15th.

```{r, message = FALSE}
library(covidcast)
library(dplyr)

start_day <- "2020-03-01"
end_day <- "2020-08-15"

inum <- suppressMessages(
  covidcast_signal(data_source = "jhu-csse",
                   signal = "confirmed_7dav_incidence_num", 
                   start_day = start_day, end_day = end_day)
)
summary(inum)

dnum <- suppressMessages(
  covidcast_signal(data_source = "jhu-csse",
                   signal = "deaths_7dav_incidence_num", 
                   start_day = start_day, end_day = end_day)
)
summary(dnum)

# Restrict attention to "active" counties with at least 500 total cases 
case_num <- 500
geo_values <- inum %>% group_by(geo_value) %>% 
  summarize(total = sum(value)) %>% 
  filter(total >= case_num) %>% pull(geo_value)
inum_act <- inum %>% filter(geo_value %in% geo_values)
dnum_act <- dnum %>% filter(geo_value %in% geo_values)
```

## Correlations sliced by time

```{r, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
```

The `covidcast_cor()` function is your primary way to calculate correlations.
The first option we have is to "slice by time": this calculates, for each time, 
correlation between the signals over all geographic locations. This is obtained
by setting `by = "time_value"`:

```{r, warning = FALSE}
library(ggplot2)

# Compute correlation per time, over all counties
df_cor1 <- covidcast_cor(inum_act, dnum_act, by = "time_value")

# Plot the correlation time series
ggplot(df_cor1, aes(x = time_value, y = value)) + geom_line() + 
  labs(title = "Correlation between cases and deaths",
       subtitle = sprintf("Per day, over counties with at least %i cases", 
                          case_num),
       x = "Date", y = "Correlation") 
```

(The sudden drop on July 25th is due to a [sudden change in how New Jersey
reported deaths](https://github.com/CSSEGISandData/COVID-19/issues/2763) being
reflected in our data source as large outliers; since the signal is a 7-day
average, these outliers last until the beginning of July and affect the reported
correlation.)

We might also be interested in how cases now correlate with deaths in the
*future*. Using the `dt_x` parameter, we can lag cases back 10 days in time, 
before calculating correlations:

```{r, warning = FALSE}
# Same, but now lag incidence case numbers back 10 days in time
df_cor2 <- covidcast_cor(inum_act, dnum_act, by = "time_value", dt_x = -10)

# Stack rowwise into one data frame, then plot time series
df_cor <- rbind(df_cor1, df_cor2)
df_cor$dt <- as.factor(c(rep(0, nrow(df_cor1)), rep(-10, nrow(df_cor2))))
ggplot(df_cor, aes(x = time_value, y = value)) + 
  geom_line(aes(color = dt)) + 
  labs(title = "Correlation between cases and deaths",
       subtitle = sprintf("Per day, over counties with at least %i cases", 
                          case_num),
       x = "Date", y = "Correlation") + 
  theme(legend.position = "bottom")
```

We can see that, for the most part, lagging the cases time series back by 10
days improves correlations, showing that cases are better correlated with deaths
10 days from now.

We can also look at Spearman (rank) correlation, which is a more robust measure 
of correlation: it's invariant to monotone transformations, and doesn't rely on
any particular functional form for the dependence between two variables.

```{r, warning = FALSE}
# Repeat this comparison, but now using Spearman (rank) correlation
df_cor1 <- covidcast_cor(inum_act, dnum_act, by = "time_value", 
                        method = "spearman")
df_cor2 <- covidcast_cor(inum_act, dnum_act, by = "time_value", dt_x = -10,
                        method = "spearman")

# Stack rowwise into one data frame, then plot time series
df_cor <- rbind(df_cor1, df_cor2)
df_cor$dt <- as.factor(c(rep(0, nrow(df_cor1)), rep(-10, nrow(df_cor2))))
ggplot(df_cor, aes(x = time_value, y = value)) + 
  geom_line(aes(color = dt)) + 
  labs(title = "Correlation between cases and deaths",
       subtitle = sprintf("Per day, over counties with at least %i cases", 
                          case_num), 
       x = "Date", y = "Correlation") +
  theme(legend.position = "bottom")
```

The "big dip" is gone (since the Spearman correlation uses ranks and not the
actual values, and hence is less sensitive to outliers), and we can again see
that lagging the cases time series helps correlations.

## Correlations sliced by county

The second option we have is to "slice by location": this calculates, for each 
geographic location, correlation between the time series of two signals. This
is obtained by setting `by = "geo_value"`. We'll again look at correlations 
both for observations at the same time and for 10-day lagged cases:

```{r, warning = FALSE}
# Compute correlation per county, over all times
df_cor1 <- covidcast_cor(inum_act, dnum_act, by = "geo_value")
df_cor2 <- covidcast_cor(inum_act, dnum_act, by = "geo_value", dt_x = -10)

# Stack rowwise into one data frame, then plot densities
df_cor <- rbind(df_cor1, df_cor2)
df_cor$dt <- as.factor(c(rep(0, nrow(df_cor1)), rep(-10, nrow(df_cor2))))
ggplot(df_cor, aes(value)) + 
  geom_density(aes(color = dt, fill = dt), alpha = 0.5) + 
  labs(title = "Correlation between cases and deaths",
       subtitle = "Computed separately for each county, over all times",
       x = "Date", y = "Density") +
  theme(legend.position = "bottom")
```

Using some tricks, we can attach the necessary properties to the data frame so
we can plot these correlations in space as a choropleth map, using
`plot.covidcast_signal()`:

```{r, fig.width = 10, fig.height = 8}
# Set a bunch of fields so that the data frame knows how to plot itself
df_cor2$time_value <- start_day
df_cor2$issue <- start_day
attributes(df_cor2)$metadata$geo_type <- "county"
class(df_cor2) <- c("covidcast_signal", "data.frame")

# Plot choropleth maps, using the covidcast plotting functionality
plot(df_cor2, title = "Correlations between 10-day lagged cases and deaths",
     range = c(-1, 1), choro_col = c("orange","lightblue", "purple"))
```

## More systematic lag analysis

You could also imagine trying to move the signals with various lags to see at
what lag one signal is most correlated with the other. A simple way to achieve
this:

```{r, message = TRUE, warning = FALSE, fig.width = 6, fig.height = 4}
# Loop over values for dt, and compute correlations per county
dt_vec <- -(0:15)
df_list <- vector("list", length(dt_vec))
for (i in 1:length(dt_vec)) {
  df_list[[i]] <- covidcast_cor(inum_act, dnum_act, dt_x = dt_vec[i],
                               by = "geo_value")
  df_list[[i]]$dt <- dt_vec[i]
}

# Stack into one big data frame, and then plot the median correlation by dt
df <- do.call(rbind, df_list)
df %>%
  group_by(dt) %>%
  summarize(median = median(value, na.rm = TRUE), .groups = "drop_last") %>%
  ggplot(aes(x = dt, y = median)) + geom_line() + geom_point() +
  labs(title = "Median correlation between cases and deaths",
       x = "dt", y = "Correlation") +
  theme(legend.position = "bottom", legend.title = element_blank())
```

We can see that the median correlation between cases and deaths (where the
correlations come from slicing by location) is maximized when we lag the case
incidence numbers back 8 days in time.
