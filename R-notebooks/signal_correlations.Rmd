---
title: "Basic Correlation Analyses for COVID-19 Indicators"
author: "Delphi Group"
date: "April 30, 2020"
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=4)
```

Getting data from the Epidata API
===

First we're going to fetch our indicators from the [covidcast endpoint](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html) of our [Epidata API](https://cmu-delphi.github.io/delphi-epidata/api/). 

```{r}
# This is where the basic R functionality lives
source("https://raw.githubusercontent.com/cmu-delphi/delphi-epidata/master/src/client/delphi_epidata.R")

# Helper function for fetching data from the API
get_data = function(sources, signals, dates) {
  N = length(sources)
  signal_dfs = vector(mode="list", length=N)
  for (i in 1:N) {
    cat(sprintf("(%i of %i) Fetching %s : %s ...\n", i, N, sources[i], signals[i]))
    res_county = Epidata$covidcast(sources[i], signals[i], "day", "county", dates, "*")
    res_msa = Epidata$covidcast(sources[i], signals[i], "day", "msa", dates, "*")

    # Lump together, change all NULL values to NA, reformat to a matrix
    dat = c(res_county$epidata, res_msa$epidata)
    dat = lapply(dat, function(x) lapply(x, function(v) ifelse (is.null(v), NA, v)))
    signal_dfs[[i]] = as.data.frame(matrix(as.numeric(unlist(dat)), nrow=length(dat), byrow=TRUE))
    signal_dfs[[i]] = cbind(signal_dfs[[i]], c(rep(TRUE, length(res_county$epidata)), rep(FALSE, length(res_msa$epidata))))
    colnames(signal_dfs[[i]]) = c(names(dat[[1]]), "is_county")
  }
  if (N == 1) signal_dfs = signal_dfs[[1]] 
  return(signal_dfs)
}

# Fetch the following sources and signals
sources = c("doctor-visits", "fb-survey", "google-survey", "ght")
signals = c("smoothed_cli", "smoothed_cli", "smoothed_cli", "smoothed_search")
dates = Epidata$range("20200411", "20200417") # Format is YYYYMMDD
signal_dfs = get_data(sources, signals, dates)

# Fetch incident confirmed cases
cases = get_data("jhu-cases", "confirmed_incidence", dates)
```

Compute rank correlations, county level
===

Now for each one of our indicators, we're going to:  

1. Average this indicator over a 1 week period, for each county in which this indicator is available, giving us a vector $y$.
2. Average the daily incidence of COVID-19 cases (new cases per 100,000 people) as reported in the JHU CSSE COVID-19 GitHub over the same 1 week period, for each county in which this is available, giving us a vector $x$.
3. Find the commonly available counties between $x$ and $y$, and call the restrictions of $x$ and $y$ to this common set $\tilde{x}$ and $\tilde{y}$.
4. Compute the Spearman or rank correlation between $\tilde{x}$ and $\tilde{y}$ (the usual Pearson correlation between $\mathrm{rank}(\tilde{x})$ and $\mathrm{rank}(\tilde{y})$, the vectors of ranks of $\tilde{x}$ and $\tilde{y}$).  

To elaborate, in Step 4 we're actually going to compute the rank correlation for each *population sweep cut* of $\tilde{x}$ and $\tilde{y}$.  

- A population cut of $\tilde{x}$ and $\tilde{y}$ means that we will further subset these vectors to only consider counties that have population at least $p$.  
- A population sweep cut means that we will consider all population cuts over all possible values of $p$.

In the plots below, the horizontal axis shows the population threshold (on a log scale), the vertical axis shows the corresponding rank correlation. (The top horizontal axis below the plot title shows the number of counties that survived the population cut, and were used to compute the rank correlation.)

```{r}
library(dplyr)

# Handy function for plotting rank correlation of x and y, filtering over various population thresholds
cor_plot = function(x, y, pop_df, min_obs = 100, main=NULL) {
  # Join by geo identifier
  z = inner_join(x, y, by="geo_value")
    
  # Filter out low population locations
  z = inner_join(z, pop_df, by="geo_value") %>%
    arrange(population)
    
  n = nrow(z); if (n < min_obs) { plot.new(); return() }
  pop_vec = z$population[(min_obs+1):n]
  cor_vec = rep(NA, n-min_obs)
  for (i in 1:(n-min_obs)) {
    cor_vec[i] = cor(z$value.x[i:n], z$value.y[i:n], method="spearman")
  }
  
  plot(pop_vec, cor_vec, type="l", log="x", main=main,
       xlab="Population threshold", ylab="Rank correlation")
  axis(3, at=pop_vec, labels=n:(min_obs+1), cex.axis=0.8)
  invisible(list(pop_vec=pop_vec, cor_vec=cor_vec))
}

# Get county population estimates from 2019
county_pop = read.csv("https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv")
county_pop$geo_value = county_pop$STATE * 1000 + county_pop$COUNTY # Create 5-digit FIPS code
county_pop$population = county_pop$POPESTIMATE2019 # Create population column

# Compute pairwise rank correlations
inds_county = which(sapply(signal_dfs, function(df) sum(df$is_county) > 0))
N_county = length(inds_county)
par(mfrow=c(1, N_county), mar=c(4.5, 4.5, 5.5, 0.5))

# Average over available weeks within each county
x = cases %>% 
  filter(is_county) %>%
  group_by(geo_value) %>%
  summarize(value = mean(value, na.rm=TRUE))

for (i in inds_county) {
  # Average over available weeks within each county 
  y = signal_dfs[[i]] %>% 
    filter(is_county) %>%
    group_by(geo_value) %>%
    summarize(value = mean(value, na.rm=TRUE))
    
  # Compute and plot rank correlations by pop threshold
  cor_plot(x, y, county_pop, main=sources[i])
}
```

Compute rank correlations, MSA level
===

Same as in the last section, but for metropolitan statistical areas or MSAs, instead of counties.

```{r}
# Get MSA population estimates from 2019
msa_pop = read.csv("https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/metro/totals/cbsa-est2019-alldata.csv")
msa_pop$geo_value = msa_pop$CBSA # Grab the CBSA code
msa_pop$population = msa_pop$POPESTIMATE2019 # Create population column

# Compute pairwise rank correlations
inds_msa = which(sapply(signal_dfs, function(df) sum(!df$is_county) > 0))
N_msa = length(inds_msa)
par(mfrow=c(1, N_msa), mar=c(4.5, 4.5, 5.5, 0.5))

# Average over available weeks within each MSA 
x = cases %>% 
  filter(!is_county) %>%
  group_by(geo_value) %>%
  summarize(value = mean(value, na.rm=TRUE))

for (i in inds_msa) {
  # Average over available weeks within each MSA 
  y = signal_dfs[[i]] %>% 
    filter(!is_county) %>%
    group_by(geo_value) %>%
    summarize(value = mean(value, na.rm=TRUE))
    
  # Compute and plot rank correlations by pop threshold
  cor_plot(x, y, msa_pop, main=sources[i])
}
```

Correlations to 1-week-ahead JHU cases
===

Same as in the last section, but now computing rank correlations against daily incident cases averaged over the *next* week (relative to the week we consider for our indicators).

```{r}
# Fetch 1-week-ahead JHU cases
dates_ahead = Epidata$range("20200418", "20200424") # Format is YYYYMMDD
cases_ahead = get_data("jhu-cases", "confirmed_incidence", dates_ahead)
par(mfrow=c(1, N_county), mar=c(4.5, 4.5, 5.5, 0.5))

# Average over available weeks per county
x = cases_ahead %>% 
  filter(is_county) %>%
  group_by(geo_value) %>%
  summarize(value = mean(value, na.rm=TRUE))

for (i in inds_county) {
# Average over available weeks per county
  y = signal_dfs[[i]] %>% 
    filter(is_county) %>%
    group_by(geo_value) %>%
    summarize(value = mean(value, na.rm=TRUE))
    
  # Compute and plot rank correlations by pop threshold
  cor_plot(x, y, county_pop, main=sources[i])
}

par(mfrow=c(1, N_msa), mar=c(4.5, 4.5, 5.5, 0.5))

# Average over available weeks per MSA
x = cases_ahead %>% 
  filter(!is_county) %>%
  group_by(geo_value) %>%
  summarize(value = mean(value, na.rm=TRUE))

for (i in inds_msa) {
  # Average over available weeks per MSA
  y = signal_dfs[[i]] %>% 
    filter(!is_county) %>%
    group_by(geo_value) %>%
    summarize(value = mean(value, na.rm=TRUE))
    
  # Compute and plot rank correlations by pop threshold
  cor_plot(x, y, msa_pop, main=sources[i])
}
```

Correlations to 1-week-ahead JHU deaths
===

Same as in the last section, but now computing rank correlations against daily incident deaths averaged over the *next* week (relative to the week we consider for our indicators).

```{r}
# Fetch 1-week-ahead JHU cases
dates_ahead = Epidata$range("20200418", "20200424") # Format is YYYYMMDD
deaths_ahead = get_data("jhu-cases", "deaths_incidence", dates_ahead)
par(mfrow=c(1, N_county), mar=c(4.5, 4.5, 5.5, 0.5))

# Average over available weeks per county
x = deaths_ahead %>% 
  filter(is_county) %>%
  group_by(geo_value) %>%
  summarize(value = mean(value, na.rm=TRUE))

for (i in inds_county) {
# Average over available weeks per county
  y = signal_dfs[[i]] %>% 
    filter(is_county) %>%
    group_by(geo_value) %>%
    summarize(value = mean(value, na.rm=TRUE))
    
  # Compute and plot rank correlations by pop threshold
  cor_plot(x, y, county_pop, main=sources[i])
}

par(mfrow=c(1, N_msa), mar=c(4.5, 4.5, 5.5, 0.5))

# Average over available weeks per MSA
x = deaths_ahead %>% 
  filter(!is_county) %>%
  group_by(geo_value) %>%
  summarize(value = mean(value, na.rm=TRUE))

for (i in inds_msa) {
  # Average over available weeks per MSA
  y = signal_dfs[[i]] %>% 
    filter(!is_county) %>%
    group_by(geo_value) %>%
    summarize(value = mean(value, na.rm=TRUE))
    
  # Compute and plot rank correlations by pop threshold
  cor_plot(x, y, msa_pop, main=sources[i])
}
```
